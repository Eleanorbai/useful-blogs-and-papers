{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.cbdio.com/image/attachement/png/site2/20151015/1326107595190127295.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与图像和语音不同，语言通常被认为具有明显的树结构，也就是说，在进行语法或语义组合时，通常不是按照词的顺序进行组合，而是先组合语法或语义关系比较近的词或短语。如句子：\n",
    "\n",
    "我 喜欢 红 苹果 。\n",
    "\n",
    "其中“红”修饰的是“苹果”，“喜欢”的对象也是“苹果”而非距离较近的“红”。因此，句法分析（Syntactic Parsing）任务在自然语言处理中被认为是最核心的任务之一，并已被成功的应用于多种实际任务中，如机器翻译、问答系统、情感分析等。\n",
    "\n",
    "也是基于以上的语言学理论，在将深度神经网络（深度学习）应用于自然语言处理时，人们也自然地想到了利用语言的树结构来构建深度神经网络结构，这种网络结构又被称作递归神经网络（Recursive Neural Networks）。前面例句的句法分析结果如下图所示，而递归神经网络也是按照该分析的树结构来构造网络结构的，其中每个节点都可以使用向量进行表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.cbdio.com/image/attachement/png/site2/20151015/6664507741259189340.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "斯坦福的Socher等人（2011）最早将递归神经网络应用于情感分析任务，他们定义了如下的组合函数将两个节点（可以是词或者短语）组合成一个新的节点："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.cbdio.com/image/attachement/png/site2/20151015/3308989276372814577.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中c1和c2分别是两个子节点向量，p是父节点向量，W是权重矩阵，b是偏置向量。这里面需要注意的是，对于树中不同的组合节点，都使用同一套参数，即W和b进行组合。\n",
    "\n",
    "这种全局共用同一套组合参数的方法并不合理，因为节点的性质不同，如不同的词性、短语类型等，组合方式必然是不一样的。基于此，Socher 等人于2012和2013年分别提出了两种区分词或短语类型的模型，即SU-RNN（Syntactically-Untied RNN）和MV-RNN（Matrix-Vector RNN）。SU-RNN对不同类型的组合节点使用不同的组合参数，如ADJ与NN组合时，使用WADJ-NN。但是，相同的节点类型也未必可以共享同一套组合参数，如同样是形容词，“好”和“坏”与其它词在组合时，获得的结果应该是不同的。因此，MV-RNN将这种不同体现在每个节点上，而非不同种类的节点。它认为，每个节点除了需要使用向量表示外，还需要使用一个矩阵来表示，其中向量表示节点自身的属性，而矩阵则表示其在组合时，对另一个节点的操作。例如，“红”与“苹果”组合的结果是“红苹果”，它仍然是一种“苹果”，只是属性发生了一些改变，可以通过将“红”的矩阵与“苹果”的向量相乘获得“红苹果”的向量。\n",
    "\n",
    "然而MV-RNN对每个节点除赋予一个向量外，还需要赋予一个矩阵，其中涉及到太多的参数需要学习，所以往往学习的并不充分。为了获得更好的学习效果，Socher等人于2013年提出了RNTN（Recursive Neural Tensor Networks）模型，即使用张量（Tensor）来表示组合参数。常用的三阶张量可以被理解为多个矩阵构成的向量，其中每个矩阵可以被认为是某种类型的组合操作，最终组合在一起。通过张量，既可以减少所需学习的参数，也可以表示丰富的组合操作，因此RNTN模型也取得了较好的效果。在细粒度情感分析任务上，将MV-RNN获得的44.4%的准确率，提高到了45.7%。其中细粒度情感分类指的是将句子所表达的情感，如褒贬等划分为5个级别，对应的是评论网站对商品打分的星级。\n",
    "\n",
    "Socher等人以上一系列的工作都充分利用了自然语言树结构的特性，因此事先需要对输入的文本进行句法分析操作，然而由于句法分析自身并不完美，其个别错误的分析结果必然对上述模型的应用带来不良的效果。所以有一些学者，试图将模型构筑在非树结构之上，并取得了非常好的结果。如Le and Mikolov于2014年提出了Paragraph Vector模型，将著名的word2vec模型扩展到更长的文本之上，并在相同的情感分类任务上，将准确率进一步提高到48.7%。Kim于同年使用了更简单的单层CNN（Convolutional Neural Networks）模型，在Dropout等技术的帮助下，也获得了48.0%的准确率。在其它自然语言处理任务上，Zeng等人（2014）以及Zhou和Xu（2015）也在不使用句法分析的条件下，超越了基于树结构的传统模型（非深度学习模型）。\n",
    "\n",
    "以上的工作是否就证明了，面向自然语言处理的深度学习模型真的不需要树结构了呢？结论并没有那么简单，基于树结构的模型也在不断的发展，Tai等人2015年提出了Tree-LSTM模型，将序列的LSTM模型扩展到树结构上，即可以通过LSTM的忘记门机制，跳过（忘记）整棵对结果影响不大的子树，而不仅仅是一些可能没有语言学意义的子序列。由于有了树结构的帮助，就更容易对长距离节点之间的语义搭配关系进行学习，从而取得了更好的准确率（50.6%）。此工作也被Zhou和Xu（2015）所关注，并在其论文的最后说明，如果在其模型中使用树结构，也许会取得更好的效果。当然，是否会如其所愿还需要进一步的研究。\n",
    "\n",
    "关于深度学习框架下，树结构是否有效的讨论还在继续，Li等人（2015）在多个自然语言处理任务中，对序列模型（如双向LSTM）和树模型（如Tree-LSTM）进行了深入的对比。具体结果如下表所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.cbdio.com/image/attachement/png/site2/20151015/3173897335033674727.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中需要说明的是，在细粒度情感分类任务中，Li等提出可以使用标点符号这一简单的特征，将句子分割成小的片段，然后构建层次化Bi-LSTM模型，即对每个小片段使用Bi-LSTM建模，获得其表示。然后再使用一个Bi-LSTM模型将各个小片段的表示统一建模，获得整个句子的表示。经过这种简单的操作，一定程度上提高了序列模型的性能，使其又超过了树结构模型的性能。\n",
    "\n",
    "而且，还可以发现，除语义关系分类任务外，在其它任务中，序列模型都取得了比树结构模型更好的结果。那么，语义关系分类任务和其它任务何有不同呢？直观上来看，语义关系分类任务是识别两个距离较远的实体之间的语义关系，如在句子“在 哈尔滨工业大学本科生院成立典礼上，校长周玉表示，……”中，“哈尔滨工业大学”和“周玉”距离较远，他们中间的词汇往往对序列模型构成了干扰。如果能获得该句的句法分析树，则有助于去除掉这些干扰。而其它的任务，则往往通过局部的上下文就能较好的判断最终的结果，如情感分类任务，最终的结果很多时候通过情感词汇和其相邻的上下文就能识别出来，加之句法分析结果的不准确所带来的噪声，因此在这些任务上，树结构并未带来期望的收益。\n",
    "\n",
    "与此同时，Bowman等人（2015）研究结果甚至一定程度上证明，序列模型（LSTM）能够发现隐含的树结构。他们的实验方法比较有趣，使用的是人工构造的蕴含对数据集（逻辑表达式），该数据可以通过递归的方式构造，因此可以获得准确的树结构。最终的实验结论是，序列模型能够获得与树结构模型类似的准确率，当然条件是要使用更多的训练数据，论文中序列模型使用了3倍于树结构模型的训练数据。\n",
    "\n",
    "对于自然语言处理是否依赖于树结构这一问题，综合上面一系列的研究工作，我们目前能够获得的结论是：对于那些较依赖于长距离语义关系的任务，在没有充足训练数据的情况下，使用树结构模型能够获得更好的效果。当然，我们还可以从两个相反的角度来描述这一结论，即在以下两种情况下，我们无需使用树结构模型：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、我们面对的是简单问题，其较少考虑长距离的语义依赖关系；\n",
    "\n",
    "### 2、即使面对的是复杂问题，只要我们能够获得足够的训练数据；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个条件往往不以我们的意志为转移，所以只能从第二个条件想办法，即如何想办法获得足够的训练数据。请专家标注数据的老路已经行不通了，目前比较流行的是利用众包的方式获取大规模的数据。当然，对于大公司而言，还有宝贵的平台数据，如搜索引擎的日志、聊天记录等。除此之外，还可以利用大规模的自然标注数据，其实生文本自身就是非常有价值的自然标注数据，通过这个数据我们已经能够训练语言模型、词向量表示等等。最后，受到图像处理研究的启发，我们还可以利用大规模人工自动构造数据，如可以通过对原始图像进行旋转、伸缩等操作，获取更多的训练图像，在自然语言处理中，是否也可以通过对文本进行一定的变换，从而获得大规模的训练数据呢？相关的研究还处于起步阶段，相信今后会被给予更多的关注。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1].Samuel R. Bowman, Christopher D. Manning, and Christopher Potts. Tree-structured composition in neural networks without tree-structured architectures. arXiv manuscript 1506.04834. 2015.\n",
    "\n",
    "[2].Quoc Le and Tomas Mikolov. Distributed Representations of Sentences and Documents. ICML 2014.\n",
    "\n",
    "[3].Jiwei Li and Dan Jurafsky. When Are Tree Structures Necessary for Deep Learning of Representations? EMNLP 2015.\n",
    "\n",
    "[4].Yoon Kim. Convolutional Neural Networks for Sentence Classification. EMNLP 2014.\n",
    "\n",
    "[5].Richard Socher, Cliff Lin, Andrew Y. Ng, and Christopher D. Manning. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. ICML 2011.\n",
    "\n",
    "[6].Richard Socher, John Bauer, Christopher D. Manning and Andrew Y. Ng. Parsing with Compositional Vector Grammars. ACL 2013.\n",
    "\n",
    "[7].Richard Socher, Brody Huval, Christopher D. Manning and Andrew Y. Ng. Semantic Compositionality through Recursive Matrix-Vector Spaces. EMNLP 2012.\n",
    "\n",
    "[8].Richard Socher, Danqi Chen, Christopher D. Manning, Andrew Y. Ng. Reasoning With Neural Tensor Networks for Knowledge Base Completion. NIPS 2013.\n",
    "\n",
    "[9].Kai Sheng Tai, Richard Socher and Christopher D. Manning. Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks. ACL 2015.\n",
    "\n",
    "[10].Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou and Jun Zhao Relation Classification via Convolutional Deep Neural Network. Coling 2014.\n",
    "\n",
    "[11].Jie Zhou and Wei Xu. End-to-end learning of semantic role labeling using recurrent neural networks. ACL 2015.\n",
    "\n",
    "本文来自“哈工大SCIR”公众号\n",
    "\n",
    "End."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
